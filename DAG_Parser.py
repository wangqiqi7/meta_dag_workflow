import numpy as np
import pydotplus

class OffloadingTask:
    """Class representing a single offloading task node"""
    def __init__(self, id_name, processing_data_size, transmission_data_size, node_type,
                 depth=0, heft_score=0, energy=0.0, color="blue"):
        self.id_name = id_name
        self.processing_data_size = processing_data_size
        self.transmission_data_size = transmission_data_size
        self.node_type = node_type
        self.depth = depth
        self.heft_score = heft_score
        self.energy = energy
        self.color = color

class OffloadingDotParser:
    """Parser for DAGs generated by the DAG generator"""
    def __init__(self, file_name):
        self.successors = {}
        self.predecessors = {}

        # Load .gv file using pydotplus
        self.dot_obj = pydotplus.graphviz.graph_from_dot_file(file_name)
        self._parse_nodes()
        self._parse_edges()
        self._compute_depth()

    def _parse_nodes(self):
        """Parse nodes from the Graphviz object"""
        nodes = self.dot_obj.get_node_list()
        self.task_list = [0] * len(nodes)

        for node in nodes:
            node_id = node.get_name()
            attrs = node.obj_dict.get('attributes', {})
            size = int(eval(attrs.get('size', '0')))
            transmission_size = int(eval(attrs.get('latency', '0')))
            node_type = attrs.get('type', 'Scene')
            energy = float(attrs.get('energy', 0.0))
            color = attrs.get('color', 'blue')

            task = OffloadingTask(node_id, size, transmission_size, node_type, energy=energy, color=color)
            index = int(node_id) - 1
            self.task_list[index] = task

    def _parse_edges(self):
        """Parse edges and build dependency matrix"""
        edges = self.dot_obj.get_edge_list()
        self.dependencies = []
        num_tasks = len(self.task_list)
        self.dependency_matrix = np.zeros((num_tasks, num_tasks), dtype=np.float32)

        for i in range(num_tasks):
            self.predecessors[i] = []
            self.successors[i] = []
            self.dependency_matrix[i][i] = self.task_list[i].processing_data_size

        for edge in edges:
            src = int(edge.get_source()) - 1
            dst = int(edge.get_destination()) - 1
            attrs = edge.obj_dict.get('attributes', {})
            size = int(eval(attrs.get('size', '1')))

            self.predecessors[dst].append(src)
            self.successors[src].append(dst)
            self.dependency_matrix[src][dst] = size
            self.dependencies.append([src, dst, size])

    def _compute_depth(self):
        """Compute the depth of each task recursively"""
        depth_dict = {}

        def compute_depth(idx):
            if idx in depth_dict:
                return depth_dict[idx]
            if len(self.predecessors[idx]) == 0:
                depth = 0
            else:
                depth = 1 + max(compute_depth(pre) for pre in self.predecessors[idx])
            depth_dict[idx] = depth
            return depth

        for i in range(len(self.task_list)):
            self.task_list[i].depth = compute_depth(i)

    def generate_task_list(self):
        return self.task_list

    def generate_dependencies(self):
        return self.dependencies


class OffloadingTaskGraph:
    """Parsed DAG object with node/edge encoding and cost metrics"""
    def __init__(self, file_name):
        parser = OffloadingDotParser(file_name)
        self.task_list = parser.generate_task_list()
        self.num_tasks = len(self.task_list)
        self.dependency = np.zeros((self.num_tasks, self.num_tasks))
        self.pre_task_sets = [set() for _ in range(self.num_tasks)]
        self.succ_task_sets = [set() for _ in range(self.num_tasks)]
        self.edge_set = []

        # Populate dependencies
        dependencies = parser.generate_dependencies()
        for pre, succ, size in dependencies:
            self.dependency[pre][succ] = size
            self.pre_task_sets[succ].add(pre)
            self.succ_task_sets[pre].add(succ)
            self.edge_set.append([
                pre, self.task_list[pre].depth, self.task_list[pre].processing_data_size,
                size, succ, self.task_list[succ].depth, self.task_list[succ].processing_data_size
            ])

        # For feature normalization
        nonzero_values = self.dependency[self.dependency > 0.01]
        self.max_data_size = np.max(nonzero_values) if len(nonzero_values) > 0 else 1
        self.min_data_size = np.min(nonzero_values) if len(nonzero_values) > 0 else 0

        self.prioritize_sequence = []

    def normalize_feature(self, value):
        """Normalize a value between min and max data size"""
        return float(value - self.min_data_size) / float(self.max_data_size - self.min_data_size + 1e-8)

    def encode_node_sequence(self):
        """Encode node features into a fixed-size vector per node"""
        node_sequence = []
        for i in range(self.num_tasks):
            norm_proc = self.normalize_feature(self.task_list[i].processing_data_size)
            norm_trans = self.normalize_feature(self.task_list[i].transmission_data_size)
            norm_features = [norm_proc, norm_trans]

            # Predecessors and successors for fixed-length encoding
            pre_indices = [j for j in range(i) if self.dependency[j][i] > 0.1]
            succ_indices = [j for j in range(i+1, self.num_tasks) if self.dependency[i][j] > 0.1]

            while len(pre_indices) < 6:
                pre_indices.append(-1)
            while len(succ_indices) < 6:
                succ_indices.append(-1)

            pre_indices = pre_indices[:6]
            succ_indices = succ_indices[:6]

            vector = norm_features + pre_indices + succ_indices
            node_sequence.append(vector)
        return node_sequence

    def encode_node_sequence_with_ranking(self, sorted_task_ids):
        """Reorder encoded node sequence according to ranking"""
        seq = self.encode_node_sequence()
        return [seq[i] for i in sorted_task_ids]

    def encode_edge_sequence(self):
        """Return sorted edge set"""
        return sorted(self.edge_set)

    def compute_cost_metrics(self):
        """Return mean and standard deviation of dependency costs"""
        costs = self.dependency[self.dependency > 0.01]
        if len(costs) == 0:
            return 0, 0
        return np.mean(costs), np.std(costs)

    def prioritize_tasks(self, resource_cluster):
        """Compute HEFT-style priority ranking for tasks"""
        w = [0] * self.num_tasks
        for i, task in enumerate(self.task_list):
            t_local = task.processing_data_size / resource_cluster.mobile_process_capable
            t_mec = (resource_cluster.up_transmission_cost(task.processing_data_size) +
                     task.processing_data_size / resource_cluster.mec_process_capble +
                     resource_cluster.dl_transmission_cost(task.transmission_data_size))
            w[i] = min(t_local, t_mec)

        rank_dict = [-1] * self.num_tasks

        def rank(idx):
            if rank_dict[idx] != -1:
                return rank_dict[idx]
            if len(self.succ_task_sets[idx]) == 0:
                rank_dict[idx] = w[idx]
            else:
                rank_dict[idx] = w[idx] + max(rank(j) for j in self.succ_task_sets[idx])
            return rank_dict[idx]

        for i in range(self.num_tasks):
            rank(i)

        self.prioritize_sequence = list(np.argsort(rank_dict)[::-1])
        return self.prioritize_sequence
